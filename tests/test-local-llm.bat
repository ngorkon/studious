@echo off
echo ğŸ§ª Testing Local LLM Connection
echo ===============================
echo.

echo ğŸ“¡ Checking for Ollama...
curl -s http://localhost:11434/api/tags >nul 2>&1
if %errorlevel% == 0 (
    echo âœ… Ollama is running!
    echo.
    echo ğŸ“‹ Available models:
    curl -s http://localhost:11434/api/tags | findstr name
    echo.
) else (
    echo âŒ Ollama not running on port 11434
)

echo ğŸ“¡ Checking for LM Studio...
curl -s http://localhost:1234/v1/models >nul 2>&1
if %errorlevel% == 0 (
    echo âœ… LM Studio is running!
) else (
    echo âŒ LM Studio not running on port 1234
)

echo ğŸ“¡ Checking for GPT4All...
curl -s http://localhost:4891/v1/models >nul 2>&1
if %errorlevel% == 0 (
    echo âœ… GPT4All is running!
) else (
    echo âŒ GPT4All not running on port 4891
)

echo.
if %errorlevel% == 0 (
    echo ğŸ‰ Local LLM detected! Your app should work with local AI.
) else (
    echo ğŸ’¡ No local LLMs detected. Run setup-local-llm.bat to install.
)

echo.
echo ğŸš€ Start your app and check the browser console for AI provider status.
echo.
pause
