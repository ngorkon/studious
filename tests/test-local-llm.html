<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Local LLM Integration Test</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            min-height: 100vh;
        }
        .test-container {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 30px;
            margin: 20px 0;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        }
        .test-result {
            margin: 10px 0;
            padding: 10px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
        }
        .success { background: rgba(0, 255, 0, 0.2); border: 1px solid #00ff00; }
        .error { background: rgba(255, 0, 0, 0.2); border: 1px solid #ff0000; }
        .info { background: rgba(0, 150, 255, 0.2); border: 1px solid #0096ff; }
        .warning { background: rgba(255, 165, 0, 0.2); border: 1px solid #ffa500; }
        
        button {
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 25px;
            font-weight: bold;
            cursor: pointer;
            margin: 10px;
            transition: all 0.3s ease;
        }
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
        }
        
        .status-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .status-card {
            background: rgba(255, 255, 255, 0.05);
            padding: 20px;
            border-radius: 10px;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        code {
            background: rgba(0, 0, 0, 0.3);
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
    </style>
</head>
<body>
    <div class="test-container">
        <h1>🧪 Local LLM Integration Test</h1>
        <p>This page tests the local LLM integration for the Studious app.</p>
        
        <div class="status-grid">
            <div class="status-card">
                <h3>🏠 Local LLM Status</h3>
                <div id="local-llm-status">Checking...</div>
            </div>
            
            <div class="status-card">
                <h3>🔑 Environment Config</h3>
                <div id="env-config-status">Checking...</div>
            </div>
            
            <div class="status-card">
                <h3>🤖 AI Generator</h3>
                <div id="ai-generator-status">Checking...</div>
            </div>
            
            <div class="status-card">
                <h3>📄 File Loading</h3>
                <div id="file-loading-status">Checking...</div>
            </div>
        </div>
        
        <div>
            <button onclick="runTests()">🔄 Run Tests</button>
            <button onclick="testFlashcardGeneration()">🧠 Test Flashcard Generation</button>
            <button onclick="checkLocalLLMs()">🔍 Check Local LLMs</button>
            <button onclick="window.open('/', '_blank')">🚀 Open Main App</button>
        </div>
        
        <div class="test-container">
            <h3>📋 Test Results</h3>
            <div id="test-results"></div>
        </div>
        
        <div class="test-container">
            <h3>💡 Quick Setup Guide</h3>
            <p><strong>To enable local LLMs:</strong></p>
            <ol>
                <li>Install Ollama: <code>https://ollama.ai/</code></li>
                <li>Download a model: <code>ollama pull llama3:8b</code></li>
                <li>Start server: <code>ollama serve</code></li>
                <li>Refresh this page to test!</li>
            </ol>
            
            <p><strong>Alternative options:</strong></p>
            <ul>
                <li><strong>LM Studio:</strong> https://lmstudio.ai/ (GUI interface)</li>
                <li><strong>GPT4All:</strong> https://gpt4all.io/ (lightweight)</li>
            </ul>
        </div>
    </div>

    <!-- Load the required scripts -->
    <script src="assets/js/models/EnvironmentConfig.js"></script>
    <script src="assets/js/models/LocalLLMProvider.js"></script>
    
    <script>
        function logResult(message, type = 'info') {
            const resultsDiv = document.getElementById('test-results');
            const div = document.createElement('div');
            div.className = `test-result ${type}`;
            div.textContent = `${new Date().toLocaleTimeString()} - ${message}`;
            resultsDiv.appendChild(div);
            console.log(message);
        }

        async function runTests() {
            document.getElementById('test-results').innerHTML = '';
            logResult('🧪 Starting Local LLM Integration Tests...', 'info');
            
            // Test 1: LocalLLMProvider
            logResult('📋 Test 1: LocalLLMProvider Class', 'info');
            if (typeof LocalLLMProvider !== 'undefined') {
                logResult('✅ LocalLLMProvider class loaded successfully', 'success');
                
                try {
                    const llmProvider = new LocalLLMProvider();
                    logResult('✅ LocalLLMProvider instance created', 'success');
                    
                    // Wait for detection to complete
                    setTimeout(() => {
                        const status = llmProvider.getStatus();
                        document.getElementById('local-llm-status').innerHTML = 
                            status.isAvailable ? 
                            `✅ ${status.provider} (${status.model})` : 
                            '❌ No local LLMs detected';
                        
                        if (status.isAvailable) {
                            logResult(`✅ Local LLM detected: ${status.provider} (${status.model})`, 'success');
                        } else {
                            logResult('ℹ️ No local LLMs detected - Demo mode will be used', 'warning');
                        }
                    }, 2000);
                    
                } catch (error) {
                    logResult(`❌ Error creating LocalLLMProvider: ${error.message}`, 'error');
                }
            } else {
                logResult('❌ LocalLLMProvider class not found', 'error');
                document.getElementById('local-llm-status').innerHTML = '❌ Class not loaded';
            }
            
            // Test 2: EnvironmentConfig
            logResult('📋 Test 2: EnvironmentConfig Class', 'info');
            if (typeof EnvironmentConfig !== 'undefined') {
                logResult('✅ EnvironmentConfig class loaded successfully', 'success');
                
                try {
                    const envConfig = new EnvironmentConfig();
                    const keys = envConfig.getAPIKeys();
                    const hasAnyKey = !!(keys.openai || keys.anthropic || keys.google);
                    
                    document.getElementById('env-config-status').innerHTML = 
                        hasAnyKey ? '✅ API keys configured' : '⚠️ No API keys (Demo mode)';
                    
                    logResult(`🔑 API Keys: OpenAI(${!!keys.openai}) Anthropic(${!!keys.anthropic}) Google(${!!keys.google})`, 'info');
                    logResult('✅ Environment configuration working', 'success');
                } catch (error) {
                    logResult(`❌ Error with EnvironmentConfig: ${error.message}`, 'error');
                }
            } else {
                logResult('❌ EnvironmentConfig class not found', 'error');
                document.getElementById('env-config-status').innerHTML = '❌ Class not loaded';
            }
            
            // Test 3: File loading
            const requiredFiles = ['LocalLLMProvider.js', 'EnvironmentConfig.js'];
            let filesLoaded = 0;
            
            requiredFiles.forEach(file => {
                const scripts = document.querySelectorAll(`script[src*="${file}"]`);
                if (scripts.length > 0) {
                    logResult(`✅ ${file} script tag found`, 'success');
                    filesLoaded++;
                } else {
                    logResult(`❌ ${file} script tag missing`, 'error');
                }
            });
            
            document.getElementById('file-loading-status').innerHTML = 
                `${filesLoaded}/${requiredFiles.length} files loaded`;
            
            logResult('🎯 Integration test complete!', 'success');
        }

        async function testFlashcardGeneration() {
            logResult('🧠 Testing flashcard generation...', 'info');
            
            if (typeof LocalLLMProvider === 'undefined') {
                logResult('❌ LocalLLMProvider not available', 'error');
                return;
            }
            
            try {
                const llmProvider = new LocalLLMProvider();
                const testContent = "Photosynthesis is the process by which plants convert sunlight into energy using chlorophyll and carbon dioxide from the air.";
                
                // Wait for provider detection
                setTimeout(async () => {
                    try {
                        if (llmProvider.isAvailable) {
                            logResult('🏠 Attempting local LLM generation...', 'info');
                            const flashcards = await llmProvider.generateFlashcards(testContent, {
                                cardCount: 2,
                                difficulty: 'intermediate'
                            });
                            logResult(`✅ Generated ${flashcards.length} flashcards with local LLM!`, 'success');
                            logResult(`📄 Sample: ${flashcards[0].question}`, 'info');
                        } else {
                            logResult('ℹ️ No local LLM available, would use demo mode', 'warning');
                        }
                    } catch (error) {
                        logResult(`❌ Generation error: ${error.message}`, 'error');
                    }
                }, 3000);
                
            } catch (error) {
                logResult(`❌ Test setup error: ${error.message}`, 'error');
            }
        }

        async function checkLocalLLMs() {
            logResult('🔍 Checking for local LLM providers...', 'info');
            
            const providers = [
                { name: 'Ollama', url: 'http://localhost:11434/api/tags' },
                { name: 'LM Studio', url: 'http://localhost:1234/v1/models' },
                { name: 'GPT4All', url: 'http://localhost:4891/v1/models' }
            ];
            
            for (const provider of providers) {
                try {
                    const response = await fetch(provider.url, { method: 'GET' });
                    if (response.ok) {
                        logResult(`✅ ${provider.name} is running at ${provider.url}`, 'success');
                    } else {
                        logResult(`❌ ${provider.name} not responding at ${provider.url}`, 'error');
                    }
                } catch (error) {
                    logResult(`❌ ${provider.name} not available at ${provider.url}`, 'error');
                }
            }
        }

        // Run tests on page load
        document.addEventListener('DOMContentLoaded', () => {
            setTimeout(runTests, 1000);
        });
    </script>
</body>
</html>
